import asyncio
import json
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse, parse_qs

import requests
from bs4 import BeautifulSoup

from app.core.config import settings

# ë£¨íŠ¸ ê²½ë¡œì— ìˆëŠ” playwright ê¸°ë°˜ ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ sys.path ë³´ì •
ROOT_DIR = Path(__file__).resolve().parents[2]
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from naver_blog_daily import get_naver_blog_visitors  # type: ignore  # ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ í™œìš©
from naverblog import get_blog_info  # type: ignore
from naverblog_api import get_naver_blog_api  # type: ignore

class BlogService:
    def __init__(self):
        self.naver_client_id = settings.naver_client_id
        self.naver_secret_key = settings.naver_secret_key

    async def collect_blog_data(self, blog_url: str) -> Optional[Dict[str, Any]]:
        """ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            print(f"ğŸ“ Starting blog data collection for: {blog_url}")
            
            # ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            blog_data = await self._get_blog_post_info(blog_url)
            if not blog_data:
                print(f"âŒ Failed to get blog post info for: {blog_url}")
                return None

            if not blog_data.get('username'):
                blog_data['username'] = self._extract_blog_username(blog_url)

            print(f"âœ… Blog post info collected: title='{blog_data.get('title')}', username='{blog_data.get('username')}', likes={blog_data.get('likes_count')}, comments={blog_data.get('comments_count')}")

            # ì¼ì¼ ë°©ë¬¸ì ìˆ˜ ìˆ˜ì§‘
            daily_visitors = await self._get_daily_visitors(blog_url)
            blog_data['daily_visitors'] = daily_visitors
            if daily_visitors == 0:
                print(f"âš ï¸ Daily visitors count is 0 (may be due to API error)")

            blog_data['rankings'] = []
            print(f"âœ… Blog data collection completed for: {blog_url}")
            return blog_data

        except Exception as e:
            import traceback
            print(f"âŒ Error in collect_blog_data for {blog_url}: {str(e)}")
            traceback.print_exc()
            return None

    async def _get_blog_post_info(self, blog_url: str) -> Optional[Dict[str, Any]]:
        """ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” playwright ê¸°ë°˜ ìˆ˜ì§‘ìœ¼ë¡œ ëŒ€ì²´
            if 'blog.naver.com' in blog_url:
                return await self._get_naver_blog_with_playwright(blog_url)

            response = requests.get(blog_url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ íŒŒì‹±
            if 'tistory.com' in blog_url:
                return await self._parse_tistory_blog(soup, blog_url)
            else:
                # ì¼ë°˜ ë¸”ë¡œê·¸ íŒŒì‹±
                return await self._parse_general_blog(soup, blog_url)

        except Exception as e:
            print(f"Error getting blog post info: {str(e)}")
            return None

    async def _get_naver_blog_with_playwright(self, url: str) -> Dict[str, Any]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” playwright ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•´ ì‹¤ì œ ìˆ˜ì¹˜ë¥¼ ìˆ˜ì§‘"""
        loop = asyncio.get_running_loop()
        try:
            raw_info = await loop.run_in_executor(None, get_blog_info, url)
        except Exception as e:
            print(f"Error fetching Naver blog via Playwright: {str(e)}")
            import traceback
            traceback.print_exc()
            return {}

        if not raw_info:
            print(f"No data returned from get_blog_info for {url}")
            return {}

        # ì œëª© ì¶”ì¶œ (post_title ë˜ëŠ” title í‚¤ í™•ì¸)
        title = raw_info.get('title') or raw_info.get('post_title') or "ì œëª© ì—†ìŒ"
        likes_api = await self._get_like_count(url)
        # likes_count ë˜ëŠ” post_likes í‚¤ í™•ì¸
        likes = likes_api if likes_api else self._safe_int(raw_info.get('likes_count') or raw_info.get('post_likes'))
        # comments_count ë˜ëŠ” post_comments í‚¤ í™•ì¸
        comments = self._safe_int(raw_info.get('comments_count') or raw_info.get('post_comments'))
        post_date_raw = raw_info.get('post_date') or raw_info.get('posted_at')
        posted_at = self._parse_blog_date(post_date_raw) if post_date_raw else None
        username = self._extract_blog_username(url)

        return {
            'url': url,
            'title': title,
            'likes_count': likes,
            'comments_count': comments,
            'posted_at': posted_at,
            'username': username,
            'collected_at': datetime.now()
        }

    async def _parse_tistory_blog(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ íŒŒì‹±"""
        try:
            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('h1') or soup.find('h2') or soup.find('.title')
            title = title_element.get_text().strip() if title_element else "ì œëª© ì—†ìŒ"
            
            # ì¢‹ì•„ìš”/ëŒ“ê¸€ ìˆ˜ëŠ” í‹°ìŠ¤í† ë¦¬ APIë‚˜ íŠ¹ë³„í•œ ë°©ë²•ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            likes_count = 0
            comments_count = 0
            
            # í¬ìŠ¤íŒ… ë‚ ì§œ ì¶”ì¶œ
            date_element = soup.find('.date') or soup.find('time')
            posted_at = self._parse_blog_date(date_element.get_text()) if date_element else None
            
            return {
                'url': url,
                'title': title,
                'likes_count': likes_count,
                'comments_count': comments_count,
                'posted_at': posted_at,
                'collected_at': datetime.now()
            }
            
        except Exception as e:
            print(f"Error parsing Tistory blog: {str(e)}")
            return {}

    async def _parse_general_blog(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """ì¼ë°˜ ë¸”ë¡œê·¸ íŒŒì‹±"""
        try:
            # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥ì„± ì‹œë„)
            title_element = (soup.find('h1') or 
                           soup.find('h2') or 
                           soup.find('.title') or 
                           soup.find('#title') or
                           soup.find('title'))
            title = title_element.get_text().strip() if title_element else "ì œëª© ì—†ìŒ"
            
            return {
                'url': url,
                'title': title,
                'likes_count': 0,
                'comments_count': 0,
                'posted_at': None,
                'collected_at': datetime.now()
            }
            
        except Exception as e:
            print(f"Error parsing general blog: {str(e)}")
            return {}

    async def _get_daily_visitors(self, blog_url: str) -> int:
        """ì¼ì¼ ë°©ë¬¸ì ìˆ˜ ìˆ˜ì§‘ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ìš©)"""
        try:
            if 'blog.naver.com' not in blog_url:
                return 0

            api_url = self._build_naver_visitor_api_url(blog_url)
            if not api_url:
                print(f"âš ï¸ Could not build visitor API URL for: {blog_url}")
                return 0

            loop = asyncio.get_running_loop()
            raw_json = await loop.run_in_executor(None, get_naver_blog_visitors, api_url)
            if not raw_json:
                print(f"âš ï¸ No response from visitor API: {api_url}")
                return 0

            data = json.loads(raw_json)
            if not isinstance(data, dict) or not data:
                print(f"âš ï¸ Invalid visitor data format from API: {api_url}")
                return 0

            # ìµœì‹  ë‚ ì§œì˜ ë°©ë¬¸ì ìˆ˜ ë°˜í™˜
            latest_date = max(data.keys())
            latest_value = data.get(latest_date, 0)
            visitor_count = int(latest_value) if latest_value else 0
            print(f"âœ… Daily visitors collected: {visitor_count} (date: {latest_date})")
            return visitor_count

        except json.JSONDecodeError as e:
            print(f"âŒ JSON decode error for daily visitors API: {str(e)}")
            return 0
        except Exception as e:
            print(f"âŒ Error getting daily visitors: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0

    def _build_naver_visitor_api_url(self, blog_url: str) -> Optional[str]:
        username = self._extract_blog_username(blog_url)
        if not username:
            return None
        return f"https://blog.naver.com/NVisitorgp4Ajax.nhn?blogId={username}"

    def _extract_blog_username(self, blog_url: str) -> Optional[str]:
        try:
            parsed = urlparse(blog_url)
            if not parsed.netloc:
                return None

            if parsed.netloc.endswith('blog.naver.com'):
                path_parts = [part for part in parsed.path.split('/') if part]
                if path_parts:
                    return path_parts[0]

            if parsed.netloc.endswith('m.blog.naver.com'):
                path_parts = [part for part in parsed.path.split('/') if part]
                if path_parts:
                    return path_parts[0]

            # PostView.naver ë°©ì‹ ì²˜ë¦¬
            if 'blogId' in parsed.query:
                qs = parse_qs(parsed.query)
                bid = qs.get('blogId')
                if bid and bid[0]:
                    return bid[0]

            return None
        except Exception:
            return None

    def _extract_blog_log_no(self, blog_url: str) -> Optional[str]:
        try:
            parsed = urlparse(blog_url)
            path_parts = [part for part in parsed.path.split('/') if part]
            if parsed.netloc.endswith('blog.naver.com') or parsed.netloc.endswith('m.blog.naver.com'):
                if len(path_parts) >= 2:
                    return path_parts[1]

            qs = parse_qs(parsed.query)
            log_no = qs.get('logNo') or qs.get('logno')
            if log_no and log_no[0]:
                return log_no[0]

            return None
        except Exception:
            return None

    def _build_naver_entry_id(self, blog_url: str) -> Optional[str]:
        blog_id = self._extract_blog_username(blog_url)
        log_no = self._extract_blog_log_no(blog_url)
        if not blog_id or not log_no:
            return None
        return f"{blog_id}_{log_no}"

    async def _get_like_count(self, blog_url: str) -> int:
        entry_id = self._build_naver_entry_id(blog_url)
        if not entry_id:
            return 0

        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self._request_like_count, entry_id)

    def _request_like_count(self, entry_id: str) -> int:
        try:
            api_url = (
                "https://apis.naver.com/blogserver/like/v1/search/contents"
                "?suppress_response_codes=true&pool=blogid&isDuplication=false"
                f"&cssIds=MULTI_PC%2CBLOG_PC&displayId=BLOG&q=BLOG%5B{entry_id}%5D"
            )
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
                'Referer': 'https://blog.naver.com/'
            }
            response = requests.get(api_url, headers=headers, timeout=10)
            if response.status_code != 200:
                return 0

            data = response.json()
            contents = data.get('contents') if isinstance(data, dict) else None
            if not contents:
                return 0

            for item in contents:
                if not isinstance(item, dict):
                    continue
                reactions = item.get('reactions')
                if not reactions:
                    continue
                for reaction in reactions:
                    if (
                        isinstance(reaction, dict)
                        and reaction.get('reactionType') == 'like'
                        and isinstance(reaction.get('count'), (int, float))
                    ):
                        return int(reaction['count'])

            return 0
        except Exception:
            return 0

    def _normalize_blog_url(self, blog_url: str) -> Optional[str]:
        try:
            parsed = urlparse(blog_url)
            qs = parse_qs(parsed.query)

            blog_id: Optional[str] = None
            log_no: Optional[str] = None

            path_parts = [part for part in parsed.path.split('/') if part]
            if parsed.netloc.endswith('blog.naver.com') or parsed.netloc.endswith('m.blog.naver.com'):
                if len(path_parts) >= 2:
                    blog_id = path_parts[0]
                    log_no = path_parts[1]
                elif len(path_parts) >= 1:
                    blog_id = path_parts[0]

            if 'blogId' in qs and qs['blogId']:
                blog_id = blog_id or qs['blogId'][0]
            if 'logNo' in qs and qs['logNo']:
                log_no = log_no or qs['logNo'][0]

            if blog_id and log_no:
                return f"{blog_id.strip().lower()}/{log_no.strip()}"
            if blog_id:
                return blog_id.strip().lower()

            return parsed.geturl().rstrip('/')
        except Exception:
            return None

    @staticmethod
    def _safe_int(value: Any) -> int:
        try:
            if value is None:
                return 0
            if isinstance(value, int):
                result = value
            else:
                digits = re.sub(r"[^\d]", "", str(value))
                result = int(digits) if digits else 0
            # Scale down obviously malformed large numbers until within a reasonable range
            max_reasonable = 10_000
            while result > max_reasonable:
                result //= 10
                if result == 0:
                    break
            return result
        except Exception:
            return 0

    def _extract_keywords_from_title(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ì¶”ì¶œí•˜ê³  ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
            keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]+', title)
            
            # 2ê¸€ì ì´ìƒì˜ í‚¤ì›Œë“œë§Œ ì„ íƒ
            keywords = [kw for kw in keywords if len(kw) >= 2]
            
            # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì„ íƒ
            return keywords[:5]
            
        except Exception as e:
            print(f"Error extracting keywords: {str(e)}")
            return []

    async def _check_blog_ranking(self, blog_url: str, keyword: str) -> Optional[int]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ì—ì„œ í•´ë‹¹ URLì˜ ìˆœìœ„ í™•ì¸"""
        try:
            if not self.naver_client_id or not self.naver_secret_key:
                print(f"âš ï¸ Naver API credentials not configured. Cannot check ranking for keyword '{keyword}'")
                return None

            print(f"   ğŸ“¡ Calling Naver Blog API for keyword: '{keyword}'")
            loop = asyncio.get_running_loop()
            # API í‚¤ë¥¼ ì „ë‹¬í•˜ì—¬ í˜¸ì¶œ
            data = await loop.run_in_executor(
                None, 
                lambda: get_naver_blog_api(keyword, self.naver_client_id, self.naver_secret_key)
            )
            if not data or not isinstance(data, dict):
                print(f"   âŒ Invalid response data for keyword '{keyword}': {type(data)}")
                return None
            
            print(f"   âœ… Received API response for keyword '{keyword}'")

            items = data.get('items', [])
            if not isinstance(items, list):
                print(f"Invalid items data type for keyword '{keyword}': expected list, got {type(items)}")
                return None

            target_key = self._normalize_blog_url(blog_url)
            if not target_key:
                return None

            print(f"   ğŸ” Searching through {len(items)} items for blog URL: {blog_url}")
            print(f"   ğŸ” Target key: {target_key}")
            
            for i, item in enumerate(items, 1):
                if not item or not isinstance(item, dict):
                    continue
                link = item.get('link')
                if not link:
                    continue
                candidate_key = self._normalize_blog_url(link)
                if candidate_key and candidate_key == target_key:
                    print(f"   âœ… Found ranking: {i} for keyword '{keyword}'")
                    return i
                # ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œ í•­ëª©ì˜ í‚¤ ì¶œë ¥
                if i <= 3:
                    print(f"      Item {i}: {candidate_key} (from {link})")

            print(f"   âš ï¸ Blog URL not found in top 100 for keyword '{keyword}'")
            return None  # 100ìœ„ ì•ˆì— ì—†ìŒ

        except Exception as e:
            print(f"   âŒ Error checking blog ranking for keyword '{keyword}': {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _parse_blog_date(self, date_string: str) -> Optional[datetime]:
        """ë¸”ë¡œê·¸ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        
        yyyy-mm-dd í˜•ì‹ì˜ ë¬¸ìì—´ì„ ë°›ì•„ì„œ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ì´ë¯¸ yyyy-mm-dd í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ë‚ ì§œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        if not date_string:
            return None
            
        try:
            # yyyy-mm-dd í˜•ì‹ì¸ ê²½ìš° ì§ì ‘ íŒŒì‹±
            if re.match(r'^\d{4}-\d{2}-\d{2}$', date_string.strip()):
                return datetime.strptime(date_string.strip(), '%Y-%m-%d')
            
            # ì—¬ëŸ¬ ë‚ ì§œ í˜•ì‹ ì‹œë„
            date_formats = [
                '%Y.%m.%d',
                '%Y-%m-%d',
                '%Y.%m.%d.',
                '%Yë…„ %mì›” %dì¼',
                '%Y. %m. %d.',  # "2025. 12. 9." í˜•ì‹
                '%Y. %m. %d'    # "2025. 12. 9" í˜•ì‹
            ]
            
            for fmt in date_formats:
                try:
                    return datetime.strptime(date_string.strip(), fmt)
                except ValueError:
                    continue
                    
            return None
        except Exception as e:
            print(f"Error parsing date: {str(e)}")
            return None

blog_service = BlogService()
