import os
import requests
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.db.models import CampaignReelCollectionJob, CollectionJob
from app.db.database import SessionLocal
import uuid
import logging

logger = logging.getLogger(__name__)

class CampaignReelCollectionService:
    def __init__(self):
        self.api_token = os.getenv("BRIGHTDATA_API_KEY")
        self.dataset_id = "gd_lyclm20il4r5helnj"  # instagram_reels_by_url.pyì—ì„œ í™•ì¸í•œ dataset_id
        self.api_url = "https://api.brightdata.com/datasets/v3/trigger"
        
    def add_reel_collection_jobs(self, campaign_id: int, reel_urls: List[str], check_existing_data: bool = False) -> List[CampaignReelCollectionJob]:
        """ìº í˜ì¸ì˜ ë¦´ìŠ¤ URLë“¤ì„ ìˆ˜ì§‘ íì— ì¶”ê°€"""
        db = SessionLocal()
        try:
            jobs = []
            for url in reel_urls:
                # ì´ë¯¸ ìˆ˜ì§‘ ì™„ë£Œëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜µì…˜)
                if check_existing_data:
                    existing_completed_job = db.query(CampaignReelCollectionJob).filter(
                        and_(
                            CampaignReelCollectionJob.campaign_id == campaign_id,
                            CampaignReelCollectionJob.reel_url == url,
                            CampaignReelCollectionJob.status == "completed",
                            CampaignReelCollectionJob.user_posted.isnot(None)
                        )
                    ).first()
                    
                    if existing_completed_job:
                        logger.info(f"Reel data already exists for {url}, skipping")
                        continue
                
                # ì´ë¯¸ ëŒ€ê¸° ì¤‘ì´ê±°ë‚˜ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì¸ì§€ í™•ì¸
                existing_job = db.query(CampaignReelCollectionJob).filter(
                    and_(
                        CampaignReelCollectionJob.campaign_id == campaign_id,
                        CampaignReelCollectionJob.reel_url == url,
                        CampaignReelCollectionJob.status.in_(["pending", "processing"])
                    )
                ).first()
                
                if not existing_job:
                    job = CampaignReelCollectionJob(
                        campaign_id=campaign_id,
                        reel_url=url,
                        status="pending",
                        priority=1
                    )
                    db.add(job)
                    jobs.append(job)
            
            db.commit()
            logger.info(f"Added {len(jobs)} reel collection jobs for campaign {campaign_id}")
            return jobs
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error adding reel collection jobs: {str(e)}")
            raise
        finally:
            db.close()
    
    def collect_single_reel(self, reel_url: str) -> Dict:
        """ë‹¨ì¼ ë¦´ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        if not self.api_token:
            raise ValueError("BRIGHTDATA_API_KEY not found in environment variables")
            
        headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json",
        }
        
        params = {
            "dataset_id": self.dataset_id,
            "include_errors": "true",
        }
        
        data = [{"url": reel_url}]
        
        try:
            logger.info(f"Calling BrightData API for {reel_url} with dataset_id: {self.dataset_id}")
            response = requests.post(self.api_url, headers=headers, params=params, json=data, timeout=30)
            
            logger.info(f"BrightData API response status: {response.status_code}")
            response.raise_for_status()
            
            result = response.json()
            logger.info(f"BrightData API response for {reel_url}: {result}")
            
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Error calling BrightData API for {reel_url}: {str(e)}")
            raise
    
    def process_brightdata_response(self, job_id: int, brightdata_response: Dict) -> bool:
        """BrightData ì‘ë‹µì„ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        db = SessionLocal()
        try:
            job = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.id == job_id
            ).first()
            
            if not job:
                logger.error(f"Job {job_id} not found")
                return False
            
            # BrightData ì‘ë‹µì—ì„œ snapshot_id ì¶”ì¶œ
            snapshot_id = brightdata_response.get("snapshot_id")
            logger.info(f"BrightData response for job {job_id}: {brightdata_response}")
            
            if not snapshot_id:
                job.status = "failed"
                job.error_message = f"No snapshot_id in BrightData response: {brightdata_response}"
                job.completed_at = datetime.utcnow()
                db.commit()
                return False
            
            job.brightdata_job_id = snapshot_id
            job.status = "processing"
            job.started_at = datetime.utcnow()
            db.commit()
            
            # ìŠ¤ëƒ…ìƒ· ê²°ê³¼ ëŒ€ê¸° ë° ì²˜ë¦¬
            return self._wait_and_process_snapshot(job_id, snapshot_id)
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error processing BrightData response for job {job_id}: {str(e)}")
            return False
        finally:
            db.close()
    
    def _wait_and_process_snapshot(self, job_id: int, snapshot_id: str, max_wait_time: int = 180) -> bool:
        """ìŠ¤ëƒ…ìƒ· ì™„ë£Œë¥¼ ëŒ€ê¸°í•˜ê³  ê²°ê³¼ ì²˜ë¦¬ - ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ë²„ì „"""
        start_time = time.time()
        
        # BrightDataì—ì„œ ìŠ¤ëƒ…ìƒ·ì´ ì™„ë£Œë˜ì—ˆë‹¤ê³  í•˜ë‹ˆ, ë¨¼ì € ë°”ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„
        logger.info(f"Attempting immediate data download for snapshot {snapshot_id}")
        try:
            if self._fetch_and_save_reel_data(job_id, snapshot_id):
                logger.info(f"âœ… Immediate data download successful for {snapshot_id}")
                return True
        except Exception as e:
            logger.info(f"Immediate download failed, falling back to status check: {str(e)}")
        
        # ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œê°€ ì‹¤íŒ¨í•˜ë©´ ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œë„
        wait_intervals = [10, 15, 20, 30]  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        attempt = 0
        
        while time.time() - start_time < max_wait_time:
            try:
                attempt += 1
                wait_time = wait_intervals[min(attempt-1, len(wait_intervals)-1)]
                
                logger.info(f"â³ Attempt {attempt}: Checking snapshot {snapshot_id} status")
                
                # ê°„ë‹¨í•œ ìƒíƒœ í™•ì¸ - HTTP 200ì´ë©´ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„
                response_code = self._simple_snapshot_check(snapshot_id)
                
                if response_code == 200:
                    logger.info(f"ğŸ“¡ Snapshot {snapshot_id} returned 200, attempting download")
                    if self._fetch_and_save_reel_data(job_id, snapshot_id):
                        return True
                elif response_code == 202:
                    logger.info(f"â³ Snapshot {snapshot_id} still processing (202), waiting {wait_time}s")
                elif response_code == 404:
                    logger.error(f"âŒ Snapshot {snapshot_id} not found (404)")
                    self._mark_job_failed(job_id, f"Snapshot not found: {snapshot_id}")
                    return False
                else:
                    logger.warning(f"âš ï¸ Unexpected response code {response_code} for {snapshot_id}")
                
                time.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"âŒ Error checking snapshot {snapshot_id}: {str(e)}")
                time.sleep(10)
        
        # íƒ€ì„ì•„ì›ƒ
        logger.error(f"â° Timeout waiting for snapshot {snapshot_id} after {max_wait_time}s")
        self._mark_job_failed(job_id, f"Timeout waiting for BrightData snapshot after {max_wait_time}s")
        return False
    
    def _simple_snapshot_check(self, snapshot_id: str) -> int:
        """ê°„ë‹¨í•œ ìŠ¤ëƒ…ìƒ· ìƒíƒœ í™•ì¸ - HTTP ìƒíƒœ ì½”ë“œë§Œ ë°˜í™˜"""
        headers = {
            "Authorization": f"Bearer {self.api_token}",
        }
        
        status_url = f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}"
        
        try:
            response = requests.get(status_url, headers=headers, timeout=15)
            return response.status_code
        except requests.exceptions.RequestException:
            return 500  # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±
    
    def _get_snapshot_status(self, snapshot_id: str) -> str:
        """ìŠ¤ëƒ…ìƒ· ìƒíƒœ í™•ì¸ - ìƒˆë¡œìš´ BrightData API ì‘ë‹µ êµ¬ì¡° ëŒ€ì‘"""
        headers = {
            "Authorization": f"Bearer {self.api_token}",
        }
        
        status_url = f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}"
        
        try:
            logger.info(f"Checking snapshot status: {status_url}")
            response = requests.get(status_url, headers=headers, timeout=30)
            
            logger.info(f"Status check response code: {response.status_code}")
            logger.info(f"Status check response text: {response.text[:500]}")
            
            # 200: ë°ì´í„° ì¤€ë¹„ë¨ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)
            if response.status_code == 200:
                try:
                    # JSON ì‘ë‹µì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                    status_data = response.json()
                    logger.info(f"Snapshot {snapshot_id} JSON response: {status_data}")
                    
                    # ì‘ë‹µì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì™„ë£Œë¡œ íŒë‹¨
                    if isinstance(status_data, list) and len(status_data) > 0:
                        logger.info(f"Snapshot {snapshot_id} has data, marking as ready")
                        return "ready"
                    elif isinstance(status_data, dict):
                        # ìƒíƒœ í•„ë“œê°€ ìˆìœ¼ë©´ í™•ì¸
                        status = status_data.get("status", "unknown")
                        if status in ["ready", "done", "completed"]:
                            return "ready"
                        elif status in ["running", "pending", "processing"]:
                            return "running"
                        elif status in ["failed", "error"]:
                            return "failed"
                        else:
                            # ìƒíƒœ í•„ë“œê°€ ì—†ê±°ë‚˜ ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš°, ë‹¤ë¥¸ í•„ë“œë¡œ íŒë‹¨
                            if "file_urls" in status_data or "data" in status_data:
                                return "ready"
                            else:
                                return "unknown"
                    else:
                        return "unknown"
                        
                except ValueError:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° (ì˜ˆ: CSV, í…ìŠ¤íŠ¸ ë°ì´í„°)
                    if len(response.text.strip()) > 0:
                        logger.info(f"Snapshot {snapshot_id} has non-JSON data, marking as ready")
                        return "ready"
                    else:
                        return "unknown"
            
            # 202: ì²˜ë¦¬ ì¤‘
            elif response.status_code == 202:
                logger.info(f"Snapshot {snapshot_id} still processing")
                return "running"
            
            # 404: ìŠ¤ëƒ…ìƒ· ì°¾ì„ ìˆ˜ ì—†ìŒ
            elif response.status_code == 404:
                logger.error(f"Snapshot {snapshot_id} not found")
                return "failed"
            
            # ê¸°íƒ€ ì˜¤ë¥˜ ìƒíƒœ
            else:
                logger.warning(f"Snapshot status check returned {response.status_code}: {response.text[:200]}")
                # 401, 403 ë“± ì¸ì¦ ì˜¤ë¥˜ëŠ” ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                if response.status_code in [401, 403]:
                    return "failed"
                else:
                    return "unknown"
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Error checking snapshot status for {snapshot_id}: {str(e)}")
            return "unknown"
    
    def _fetch_and_save_reel_data(self, job_id: int, snapshot_id: str) -> bool:
        """ìŠ¤ëƒ…ìƒ·ì—ì„œ ë¦´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì €ì¥"""
        headers = {
            "Authorization": f"Bearer {self.api_token}",
        }
        
        # ìŠ¤ëƒ…ìƒ·ì´ ready ìƒíƒœì¼ ë•Œë§Œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        data_url = f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}?format=json"
        
        try:
            logger.info(f"Downloading data from {data_url}")
            response = requests.get(data_url, headers=headers, timeout=60)
            
            logger.info(f"Data download response code: {response.status_code}")
            logger.info(f"Data download response text preview: {response.text[:500]}")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    logger.info(f"Successfully parsed JSON data for snapshot {snapshot_id}: {len(data) if isinstance(data, list) else 1} records")
                except ValueError as e:
                    logger.error(f"Failed to parse JSON response for {snapshot_id}: {str(e)}")
                    logger.info(f"Raw response content: {response.text[:1000]}")
                    return False
            else:
                logger.error(f"Data download failed for {snapshot_id}: {response.status_code} - {response.text[:200]}")
                return False
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Error downloading snapshot data for {snapshot_id}: {str(e)}")
            return False
        
        db = SessionLocal()
        try:
            job = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.id == job_id
            ).first()
            
            if not job:
                logger.error(f"Job {job_id} not found")
                return False
            
            # ë°ì´í„°ì—ì„œ user_postedì™€ video_play_count ì¶”ì¶œ - ì¸í”Œë£¨ì–¸ì„œ ì„œë¹„ìŠ¤ ë°©ì‹ ì ìš©
            if data and len(data) > 0:
                reel_data = data[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
                
                logger.info(f"ğŸ“Š Raw reel data keys: {list(reel_data.keys()) if isinstance(reel_data, dict) else 'Not a dict'}")
                logger.info(f"ğŸ“Š Raw reel data sample: {str(reel_data)[:300]}...")
                
                # ì¸í”Œë£¨ì–¸ì„œ ì„œë¹„ìŠ¤ ë°©ì‹ìœ¼ë¡œ ê²¬ê³ í•œ ë°ì´í„° ì¶”ì¶œ
                user_posted = self._extract_user_posted(reel_data)
                video_play_count = self._extract_video_play_count(reel_data)
                thumbnail_url = self._extract_thumbnail_url(reel_data)
                
                # ì¸ë„¤ì¼ ì´ë¯¸ì§€ë¥¼ S3ì— ì—…ë¡œë“œ
                s3_thumbnail_url = None
                if thumbnail_url:
                    s3_thumbnail_url = self._upload_thumbnail_to_s3_sync(thumbnail_url, user_posted, job_id)
                
                job.user_posted = user_posted
                job.video_play_count = video_play_count
                job.thumbnail_url = thumbnail_url
                job.s3_thumbnail_url = s3_thumbnail_url
                job.status = "completed"
                job.completed_at = datetime.utcnow()
                job.job_metadata = reel_data  # ì „ì²´ ë°ì´í„° ì €ì¥
                
                logger.info(f"âœ… Successfully collected reel data for job {job_id}: user={job.user_posted}, views={job.video_play_count}")
                
                # ì¸í”Œë£¨ì–¸ì„œ í”„ë¡œí•„ ìˆ˜ì§‘ íì— ì¶”ê°€
                if job.user_posted:
                    self._add_influencer_collection_job(job.user_posted)
                
            else:
                job.status = "failed"
                job.error_message = "No data returned from BrightData"
                job.completed_at = datetime.utcnow()
            
            db.commit()
            return job.status == "completed"
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error saving reel data for job {job_id}: {str(e)}")
            return False
        finally:
            db.close()
    
    def _extract_user_posted(self, reel_data: dict) -> str:
        """ì¸í”Œë£¨ì–¸ì„œ ì„œë¹„ìŠ¤ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ìëª… ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… ì‹œë„
        possible_fields = [
            "user_posted", "username", "user", "account", 
            "profile_name", "user_name", "posted_by"
        ]
        
        for field in possible_fields:
            value = reel_data.get(field)
            if value and isinstance(value, str) and value.strip():
                logger.info(f"ğŸ“‹ Found username in field '{field}': {value}")
                return value.strip()
        
        logger.warning(f"âš ï¸ No username found in reel data. Available fields: {list(reel_data.keys())}")
        return None
    
    def _extract_video_play_count(self, reel_data: dict) -> int:
        """ì¸í”Œë£¨ì–¸ì„œ ì„œë¹„ìŠ¤ ë°©ì‹ìœ¼ë¡œ ì¬ìƒ ìˆ˜ ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… ì‹œë„
        possible_fields = [
            "video_play_count", "views", "view_count", "play_count", 
            "video_views", "total_views", "playback_count"
        ]
        
        for field in possible_fields:
            value = reel_data.get(field)
            if value is not None:
                # dict í˜•íƒœì¸ ê²½ìš° count í‚¤ í™•ì¸
                if isinstance(value, dict) and "count" in value:
                    value = value["count"]
                
                # ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
                try:
                    count = int(value)
                    logger.info(f"ğŸ“‹ Found view count in field '{field}': {count}")
                    return count
                except (TypeError, ValueError):
                    logger.warning(f"âš ï¸ Invalid view count value in field '{field}': {value}")
                    continue
        
        logger.warning(f"âš ï¸ No valid view count found in reel data. Available fields: {list(reel_data.keys())}")
        return 0
    
    def _add_influencer_collection_job(self, username: str):
        """ì¸í”Œë£¨ì–¸ì„œ í”„ë¡œí•„ ìˆ˜ì§‘ ì‘ì—…ì„ íì— ì¶”ê°€"""
        db = SessionLocal()
        try:
            # ì´ë¯¸ ìˆ˜ì§‘ ì‘ì—…ì´ ìˆëŠ”ì§€ í™•ì¸
            existing_job = db.query(CollectionJob).filter(
                and_(
                    CollectionJob.username == username,
                    CollectionJob.status.in_(["pending", "processing"])
                )
            ).first()
            
            if not existing_job:
                profile_url = f"https://www.instagram.com/{username}"
                collection_job = CollectionJob(
                    job_id=str(uuid.uuid4()),
                    url=profile_url,
                    username=username,
                    collect_profile=True,
                    collect_posts=False,  # ê²Œì‹œë¬¼ ìˆ˜ì§‘ ë¹„í™œì„±í™”
                    collect_reels=True,
                    status="pending",
                    priority=1,
                    job_metadata={"source": "campaign_reel_collection"}
                )
                
                db.add(collection_job)
                db.commit()
                
                logger.info(f"Added influencer collection job for {username}")
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error adding influencer collection job for {username}: {str(e)}")
        finally:
            db.close()
    
    def _mark_job_failed(self, job_id: int, error_message: str):
        """ì‘ì—…ì„ ì‹¤íŒ¨ë¡œ í‘œì‹œ"""
        db = SessionLocal()
        try:
            job = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.id == job_id
            ).first()
            
            if job:
                job.status = "failed"
                job.error_message = error_message
                job.completed_at = datetime.utcnow()
                db.commit()
                
        except Exception as e:
            db.rollback()
            logger.error(f"Error marking job {job_id} as failed: {str(e)}")
        finally:
            db.close()
    
    def process_pending_jobs(self, limit: int = 5, campaign_id: int = None) -> int:
        """ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…ë“¤ì„ ì²˜ë¦¬"""
        db = SessionLocal()
        try:
            query = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.status == "pending"
            )
            
            # íŠ¹ì • ìº í˜ì¸ IDê°€ ì§€ì •ëœ ê²½ìš° í•„í„°ë§
            if campaign_id is not None:
                query = query.filter(CampaignReelCollectionJob.campaign_id == campaign_id)
            
            pending_jobs = query.order_by(
                CampaignReelCollectionJob.priority.desc(),
                CampaignReelCollectionJob.created_at.asc()
            ).limit(limit).all()
            
            processed_count = 0
            for job in pending_jobs:
                try:
                    logger.info(f"Processing reel collection job {job.id} for URL: {job.reel_url}")
                    
                    # BrightData API í˜¸ì¶œ
                    brightdata_response = self.collect_single_reel(job.reel_url)
                    
                    # ì‘ë‹µ ì²˜ë¦¬
                    if self.process_brightdata_response(job.id, brightdata_response):
                        processed_count += 1
                    
                except Exception as e:
                    logger.error(f"Error processing job {job.id}: {str(e)}")
                    self._mark_job_failed(job.id, str(e))
            
            return processed_count
            
        except Exception as e:
            logger.error(f"Error processing pending jobs: {str(e)}")
            return 0
        finally:
            db.close()
    
    def retry_failed_jobs(self, campaign_id: int = None, limit: int = 5) -> int:
        """ì‹¤íŒ¨í•œ ì‘ì—…ë“¤ì„ ì¬ì‹œë„"""
        db = SessionLocal()
        try:
            query = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.status == "failed"
            )
            
            if campaign_id:
                query = query.filter(CampaignReelCollectionJob.campaign_id == campaign_id)
            
            failed_jobs = query.order_by(
                CampaignReelCollectionJob.priority.desc(),
                CampaignReelCollectionJob.created_at.asc()
            ).limit(limit).all()
            
            retried_count = 0
            for job in failed_jobs:
                try:
                    # ìƒíƒœë¥¼ pendingìœ¼ë¡œ ì´ˆê¸°í™”
                    job.status = "pending"
                    job.error_message = None
                    job.brightdata_job_id = None
                    job.started_at = None
                    job.completed_at = None
                    
                    retried_count += 1
                    logger.info(f"Retrying failed job {job.id} for URL: {job.reel_url}")
                    
                except Exception as e:
                    logger.error(f"Error retrying job {job.id}: {str(e)}")
                    continue
            
            db.commit()
            
            # ì¬ì‹œë„ í›„ ìë™ìœ¼ë¡œ ì²˜ë¦¬ ì‹œì‘
            if retried_count > 0:
                logger.info(f"Starting automatic processing of {retried_count} retried jobs")
                try:
                    # ì¬ì‹œë„ëœ ì‘ì—…ë“¤ ì¦‰ì‹œ ì²˜ë¦¬
                    processed_count = self.process_pending_jobs(limit=retried_count, campaign_id=campaign_id)
                    logger.info(f"Automatically processed {processed_count} retried jobs")
                except Exception as e:
                    logger.error(f"Error in automatic processing after retry: {str(e)}")
            
            return retried_count
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error retrying failed jobs: {str(e)}")
            return 0
        finally:
            db.close()
    
    def get_campaign_collection_status(self, campaign_id: int) -> Dict:
        """ìº í˜ì¸ì˜ ìˆ˜ì§‘ í˜„í™© ì¡°íšŒ"""
        db = SessionLocal()
        try:
            jobs = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.campaign_id == campaign_id
            ).all()
            
            status_counts = {
                "pending": 0,
                "processing": 0,
                "completed": 0,
                "failed": 0
            }
            
            for job in jobs:
                status_counts[job.status] += 1
            
            return {
                "campaign_id": campaign_id,
                "total_jobs": len(jobs),
                "status_counts": status_counts,
                "jobs": [job.to_dict() for job in jobs]
            }
            
        except Exception as e:
            logger.error(f"Error getting campaign collection status: {str(e)}")
            return {}
        finally:
            db.close()
    
    def get_all_campaigns_collection_status(self) -> List[Dict]:
        """ëª¨ë“  ìº í˜ì¸ì˜ ìˆ˜ì§‘ í˜„í™© ì¡°íšŒ"""
        db = SessionLocal()
        try:
            # ìº í˜ì¸ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í˜„í™© ì¡°íšŒ
            campaigns = db.query(CampaignReelCollectionJob.campaign_id).distinct().all()
            
            results = []
            for (campaign_id,) in campaigns:
                status = self.get_campaign_collection_status(campaign_id)
                if status:
                    results.append(status)
            
            return results
            
        except Exception as e:
            logger.error(f"Error getting all campaigns collection status: {str(e)}")
            return []
        finally:
            db.close()
    
    def cancel_processing_jobs(self, campaign_id: int = None) -> int:
        """í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ë“¤ì„ ì·¨ì†Œ"""
        db = SessionLocal()
        try:
            query = db.query(CampaignReelCollectionJob).filter(
                CampaignReelCollectionJob.status == "processing"
            )
            
            if campaign_id:
                query = query.filter(CampaignReelCollectionJob.campaign_id == campaign_id)
            
            processing_jobs = query.all()
            
            cancelled_count = 0
            for job in processing_jobs:
                try:
                    # ì²˜ë¦¬ ì¤‘ì¸ ì‘ì—…ì„ ì·¨ì†Œëœ ìƒíƒœë¡œ ë³€ê²½
                    job.status = "failed"
                    job.error_message = "Job cancelled by user"
                    job.completed_at = datetime.utcnow()
                    
                    cancelled_count += 1
                    logger.info(f"Cancelled processing job {job.id} for URL: {job.reel_url}")
                    
                except Exception as e:
                    logger.error(f"Error cancelling job {job.id}: {str(e)}")
                    continue
            
            db.commit()
            logger.info(f"Successfully cancelled {cancelled_count} processing jobs")
            return cancelled_count
            
        except Exception as e:
            db.rollback()
            logger.error(f"Error cancelling processing jobs: {str(e)}")
            return 0
        finally:
            db.close()
    
    def _extract_thumbnail_url(self, reel_data: dict) -> str:
        """ë¦´ìŠ¤ ë°ì´í„°ì—ì„œ ì¸ë„¤ì¼ URL ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… ì‹œë„
        possible_fields = [
            "thumbnail", "thumbnail_url", "cover_image", "cover_url",
            "media_thumbnail", "preview_image", "poster", "snapshot"
        ]
        
        for field in possible_fields:
            value = reel_data.get(field)
            if value and isinstance(value, str) and value.startswith("http"):
                logger.info(f"ğŸ“‹ Found thumbnail URL in field '{field}': {value}")
                return value
        
        logger.warning(f"âš ï¸ No valid thumbnail URL found in reel data. Available fields: {list(reel_data.keys())}")
        return None
    
    def _upload_thumbnail_to_s3_sync(self, thumbnail_url: str, username: str, job_id: int) -> str:
        """ì¸ë„¤ì¼ ì´ë¯¸ì§€ë¥¼ S3ì— ì—…ë¡œë“œí•˜ê³  URL ë°˜í™˜ (ë™ê¸° ë²„ì „)"""
        try:
            import requests
            import boto3
            import uuid
            from app.core.config import settings
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = requests.get(thumbnail_url, timeout=30)
            if response.status_code != 200:
                logger.error(f"âŒ Failed to download thumbnail: HTTP {response.status_code}")
                return None
            
            # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            s3_client = boto3.client(
                's3',
                aws_access_key_id=settings.s3_access_key_id,
                aws_secret_access_key=settings.s3_secret_access_key,
                region_name=settings.s3_region
            )
            
            # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
            file_extension = thumbnail_url.split('.')[-1].split('?')[0]
            if file_extension not in ['jpg', 'jpeg', 'png', 'gif']:
                file_extension = 'jpg'
            
            # S3 í‚¤ ìƒì„±
            filename = f"{uuid.uuid4().hex}.{file_extension}"
            s3_key = f"goodwave/instagram/reel/{username}/{filename}"
            
            # S3ì— ì—…ë¡œë“œ
            s3_client.put_object(
                Bucket=settings.s3_bucket,
                Key=s3_key,
                Body=response.content,
                ContentType=f'image/{file_extension}',
                ACL='public-read'
            )
            
            # ê³µê°œ URL ìƒì„±
            s3_url = f"https://{settings.s3_bucket}.s3.{settings.s3_region}.amazonaws.com/{s3_key}"
            logger.info(f"âœ… Thumbnail uploaded to S3: {s3_url}")
            return s3_url
                        
        except Exception as e:
            logger.error(f"âŒ Error uploading thumbnail to S3: {str(e)}")
            return None